{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The predictive task is to predict (or estimate how likely) whether a claim will be filed for a given yearly policy (binary classification).  \n",
    "The dataset contains a list of renters/home yearly policies in the US. Each yearly policy is marked whether a claim has been filed (`filed_claim` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMu_DfTEeaei"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQl1rM3reaek"
   },
   "outputs": [],
   "source": [
    "df_policies = pd.read_csv('data/insurance_policy_risk.zip', dtype={'policy_id': str, 'user_id': str})\n",
    "df_policies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data dictionary:**\n",
    "- `policy_id` - unique identifier of a yearly policy.\n",
    "- `user_id` - unique identifier of a user. More than one policy might be associated with the same user.\n",
    "- `state` - the insured US state. Categorical.\n",
    "- `postal_code` - the insured postal code. Categorical (some state/county information can be extracted). High Cardinality.\n",
    "- `product` - the policy type: ho4-renters / ho3-homeowners / ho6-homeowners (condo). Categorical.\n",
    "- `square_ft` - the insured property size. Numeric. 97.7% missing values.\n",
    "- `has_fire_alarm` - whether a fire alarm exists. Boolean.\n",
    "- `has_burglar_alarm` - whether a burglar alarm exists. Boolean.\n",
    "- `portable_electronics` - whether portable electronics is insured. Boolean. 3.3% missing values.\n",
    "- `coast` - Distance from the coastline. Numeric. 9.3% missing values.\n",
    "- `fire_housing_proximity` - Proximity of fire housing. Ordinal.\n",
    "- `previous_policies` - number of previous policies. Numeric.\n",
    "- `user_age` - the age of the user in years. Numeric. 0.7% missing values.\n",
    "- `card_type` - the credit card being used. Categorical. 0.4% missing values.\n",
    "- `filed_claim` - whether a user filed a claim for this policy. Boolean. Label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyCeCOuMeaek"
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fire_housing_proximity` - this field was not resolved to `int` type as expected (not included in `describe` dump).  \n",
    "There are 15 instances with a character attached to a valid numeric value. It doesn't seem to correlate with anything.   \n",
    "Removing it to keep the ordinal nature of the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBbXYGGNeaek"
   },
   "outputs": [],
   "source": [
    "df_policies['fire_housing_proximity'] = df_policies['fire_housing_proximity'].str.extract('(\\d)').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a free postal code dataset and keep it in `data` folder.  \n",
    "Enrich the high-cardinality postal code with the following data:\n",
    "- `zip` - the 5-digit zip code assigned by the U.S. Postal Service\n",
    "- `lat` - the latitude of the zip code\n",
    "- `lng` - the longitude of the zip code\n",
    "- `city` - the official USPS city name\n",
    "- `population` - an estimate of the zip code's population\n",
    "- `density` - the estimated population per square kilometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f data/uszips.csv ]; then\n",
    "    wget https://simplemaps.com/static/data/us-zips/1.84/basic/simplemaps_uszips_basicv1.84.zip -O data/uszips.zip\n",
    "    unzip data/uszips.zip -d data\n",
    "else\n",
    "    echo \"File 'data/unzips.csv' already exists. Skipping download.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uszips = pd.read_csv('data/uszips.csv', index_col='zip', usecols=['zip','lat','lng','city','population','density'])\n",
    "\n",
    "# Impute missing zip codes with adjusted ones\n",
    "df_uszips.loc[10270] = df_uszips.loc[10128].copy()\n",
    "df_uszips.loc[60699] = df_uszips.loc[60607].copy()\n",
    "\n",
    "df = df_policies.join(df_uszips, on='postal_code', how='inner')\n",
    "\n",
    "# Verify all postal codes (of the policies) have been resolved\n",
    "assert len(df) == len(df_policies)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To prevent data leakage, all samples of the same user are within the same fold.\n",
    "1. Due to the small number of `True` labels, the label ratio is kept between the folds.\n",
    "1. The dataset is split before the EDA so any of the decisions will not rely on the data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADiAIWofeaez"
   },
   "outputs": [],
   "source": [
    "# To create a test dataset with the above directives (user in one fold, keep label ratio):\n",
    "# Constructing users list with aggregated target (True if one of the user samples is True, otherwise False).\n",
    "user_ids, labels = zip(\n",
    "    *[\n",
    "        (user_id, np.any(user_rows['filed_claim'])) for user_id, user_rows in df.groupby('user_id')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the users (70/30) while keeping the proportion of the labels (stratify).\n",
    "users_train, users_test = train_test_split(user_ids, test_size=0.3, stratify=labels, random_state=42)\n",
    "\n",
    "df_train = df[df.user_id.isin(users_train)]\n",
    "df_test = df[df.user_id.isin(users_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate train/test split.\n",
    "print(f\"Split size: {len(df_train)} + {len(df_test)} = {len(df)}\")\n",
    "print(f\"Number of users exist in both groups: {len(set(df_train['user_id']) & set(df_test['user_id']))}\")\n",
    "print(\"True target ratio in test: {:.1f}%\".format(len(df_test[df_test.filed_claim]) / 281 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`97.8%` of the policies are `ho4` (renters).  \n",
    "Claim ratio of `ho3` (homeowners) and `ho6` (condo) is a little higher, but it is not statistically significant due to the small sample size (pvalue: 0.21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claimed_contingency_table(field):\n",
    "    contingency_table = df_train.groupby(field, observed=True).agg(total=('filed_claim', 'count'), claimed=('filed_claim', 'sum'))\n",
    "    contingency_table['unclaimed'] = contingency_table['total'] - contingency_table['claimed']\n",
    "    contingency_table['ratio'] = contingency_table['claimed'] / contingency_table['total']\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_contingency_table = create_claimed_contingency_table('product')\n",
    "product_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(product_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the policies are sold in `CA`, `TX`, and `NY`.  \n",
    "There is no statistically significant claim ratio difference between the different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_contingency_table = create_claimed_contingency_table('state')\n",
    "state_contingency_table = state_contingency_table.sort_values('total', ascending=False).head(7)\n",
    "state_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = stats.chi2_contingency(state_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He9jrgveeaet",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## user_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is a skew to the left in the distribution of user ages. This skew indicates that there are more samples with younger ages. The median age is 28.  \n",
    "Then, we use `qcut` to split the samples into 5 bins with similar sizes.  \n",
    "Examining the claim frequency for each age range reveals that users in the ages of 27 to 30 have the highest claim frequency. Claim frequency is lower the younger/older a user is (pvalue: 0.038).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['user_age'], bins=range(17, 80), edgecolor='black')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of User Age')\n",
    "plt.xticks(range(15, 80, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['user_age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['age_bin'], age_bins = pd.qcut(df_train['user_age'], q=5, precision=0, retbins=True)\n",
    "age_bin_contingency_table = create_claimed_contingency_table('age_bin')\n",
    "claim_ratio = age_bin_contingency_table['ratio']\n",
    "claim_ratio.index = claim_ratio.index.astype(str).str.replace('\\.0', '', regex=True)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(claim_ratio)), claim_ratio)\n",
    "plt.xlabel('Age Bins')\n",
    "plt.ylabel('Claim Ratio')\n",
    "plt.title('Claim Ratio by Age')\n",
    "plt.xticks(range(len(claim_ratio)), claim_ratio.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(age_bin_contingency_table[['claimed', 'unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### *Is card type correlative with claimed policies?* **Yes**  \n",
    "Policies with different card types have different likelihood to be claimed. Chi-squared test Null hypothesis has been rejected (pvalue: 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_type_contingency_table = create_claimed_contingency_table('card_type')\n",
    "card_type_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(card_type_contingency_table[['claimed','unclaimed']], correction=False)\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhaQllnqeaes",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## coast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coast` field represents the distance from the coastline.  \n",
    "Varies from 0 (San Francisco) to 30 (Sacramento).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxi8sMvJeaes",
    "outputId": "8bf60514-427c-47cb-cec2-a645fb7f529a"
   },
   "outputs": [],
   "source": [
    "for coast, records in df_train[df_train.state == 'CA'].groupby('coast'):\n",
    "    first = records.iloc[0]\n",
    "    zipcode_info = df_uszips.loc[first.postal_code]\n",
    "    print(f'{int(first.coast)}: {zipcode_info.city}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are attributed to inland states.  \n",
    "Impute the missing values with 30, which reflects the same coastline risk as the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coast is null in inland states: \" + ','.join(df_train[df_train.coast.isnull()].state.unique()))\n",
    "print(\"coast is not null in states with coasts: \" + ','.join(df_train[df_train.coast.notnull()].state.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is distance from the coastline correlative with claimed policies?* **No.**  \n",
    "No statistical difference between claimed/unclaimed ratios. Chi-squared test Null hypothesis has not been rejected (pvalue: 0.15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coast_contingency_table = create_claimed_contingency_table('coast')\n",
    "coast_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(coast_contingency_table[['claimed','unclaimed']], correction=False)\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## has_fire_alarm | has_burglar_alarm | portable_electronics | previous_policies (binary features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the binary features does not seem to have a significant correlation with `filed_claim`.  \n",
    "The binary features also don't correlate between themselves, except for `has_fire_alarm` and `has_burglar_alarm` features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_train[['has_fire_alarm','has_burglar_alarm','portable_electronics','previous_policies','filed_claim']].corr()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix Heatmap of Boolean Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## fire_housing_proximity + has_fire_alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fire-housing proximity value is clipped at `4`, because there is not enough data for values 4+.  \n",
    "No statistical difference between claimed/unclaimed ratios. Chi-squared test Null hypothesis has not been rejected (pvalue: 0.59)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fire_housing_proximity_clipped'] = df_train['fire_housing_proximity'].clip(upper=3)\n",
    "fire_housing_contingency_table = df_train.groupby(['fire_housing_proximity_clipped', 'has_fire_alarm'], observed=True).agg(\n",
    "    total=('filed_claim', 'count'), claimed=('filed_claim', 'sum')\n",
    ")\n",
    "fire_housing_contingency_table['unclaimed'] = fire_housing_contingency_table['total'] - fire_housing_contingency_table['claimed']\n",
    "fire_housing_contingency_table['ratio'] = fire_housing_contingency_table['claimed'] / fire_housing_contingency_table['total']\n",
    "fire_housing_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(fire_housing_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## portable_electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies with coverage for portable electronics are more likely to be claimed.  \n",
    "Interestingly, there is a very high correlation of missing value to `filed_claim`. The mechanism behind the missing value is not clear, but we might want to keep this indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['portable_electronics_imputed'] = df_train['portable_electronics'].fillna('missing')\n",
    "portable_electronics_contingency_table = create_claimed_contingency_table('portable_electronics_imputed')\n",
    "portable_electronics_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(portable_electronics_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_user_age_count = df_train['user_age'].isnull().sum()\n",
    "missing_portable_electronics_count = df_train['portable_electronics'].isnull().sum()\n",
    "missing_user_age_and_portable_electronics_count = (df_train['portable_electronics'].isnull() & df_train['user_age'].isnull()).sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "user_age missing values are correlated with portable_electronics missingness.\n",
    "Overall user_age missing values: {missing_user_age_count} ({round(missing_user_age_count / len(df_train) * 100, 1)}% of total).\n",
    "If portable_electronics is missing: 52 ({\n",
    "    round(missing_user_age_and_portable_electronics_count / missing_portable_electronics_count * 100, 1)\n",
    "    }% of {missing_portable_electronics_count} records with missing portable_electronics values).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut_YLQGJeaeu"
   },
   "source": [
    "- `card_type` - missing values cannot be attributed to any cause. Assuming MCAR missing mechanism. Impute with the most frequent value (`debit`).\n",
    "- `coast` - missing values are attributed to inland states (see EDA) (MAR missing mechanism). Impute with `30`, which is the maximum value (most far from the coastline).\n",
    "- `user_age` - missing values are correlated with `portable_electronics` missingness (see above), so there is an underline cause that is unknown and likely related to the data collection (MAR missing mechanism). Impute with median age.\n",
    "- `portable_electronics` (see `user_age`) - since the missing value is more indicative of filing a claim, it will be imputed with value `1.0` which is also correlated with claim filing.\n",
    "- `square_ft` - missing value highly correlated with `ho4` (renter) product (MAR missing mechanism). Dropping this feature, since 97.7% of the values are missing and the claim ratio of the remaining data is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = df_train['user_age'].median()\n",
    "\n",
    "def impute_missing_data(_df):\n",
    "    _df.fillna(\n",
    "        {\n",
    "            'card_type': 'debit',\n",
    "            'coast': 30,\n",
    "            'user_age': median_age,\n",
    "            'portable_electronics': 1.0,\n",
    "        },\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Claim History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_previous_claims_feature(_df):\n",
    "    previous_claims = 0\n",
    "    prev_user_id = None\n",
    "    for ix, row in _df.sort_values(['user_id', 'previous_policies']).iterrows():\n",
    "        user_id = row['user_id']\n",
    "        if prev_user_id != user_id:\n",
    "            previous_claims = 0\n",
    "            \n",
    "        _df.loc[ix, 'previous_claims'] = previous_claims\n",
    "        previous_claims += row['filed_claim']\n",
    "        prev_user_id = user_id\n",
    "\n",
    "prepare_previous_claims_feature(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_claims_contingency_table = create_claimed_contingency_table('previous_claims')\n",
    "previous_claims_contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['population'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of User Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['population_bin'], population_bins = pd.qcut(df_train['population'], q=10, precision=0, retbins=True)\n",
    "population_bin_contingency_table = create_claimed_contingency_table('population_bin')\n",
    "claim_ratio = population_bin_contingency_table['ratio']\n",
    "claim_ratio.index = claim_ratio.index.astype(str).str.replace('\\.0', '', regex=True)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(claim_ratio)), claim_ratio)\n",
    "plt.xlabel('Age Bins')\n",
    "plt.ylabel('Claim Ratio')\n",
    "plt.title('Claim Ratio by Age')\n",
    "plt.xticks(range(len(claim_ratio)), claim_ratio.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_create_claimed_contingency_table(field):\n",
    "    contingency_table = df_test.groupby(field, observed=True).agg(total=('filed_claim', 'count'), claimed=('filed_claim', 'sum'))\n",
    "    contingency_table['unclaimed'] = contingency_table['total'] - contingency_table['claimed']\n",
    "    contingency_table['ratio'] = contingency_table['claimed'] / contingency_table['total']\n",
    "    return contingency_table\n",
    "    \n",
    "df_test['population_bin'], population_bins = pd.qcut(df_test['population'], q=10, precision=0, retbins=True)\n",
    "population_bin_contingency_table = test_create_claimed_contingency_table('population_bin')\n",
    "claim_ratio = population_bin_contingency_table['ratio']\n",
    "claim_ratio.index = claim_ratio.index.astype(str).str.replace('\\.0', '', regex=True)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(claim_ratio)), claim_ratio)\n",
    "plt.xlabel('Age Bins')\n",
    "plt.ylabel('Claim Ratio')\n",
    "plt.title('Claim Ratio by Age')\n",
    "plt.xticks(range(len(claim_ratio)), claim_ratio.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(population_bin_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['density_bin'], density_bins = pd.qcut(df_train['density'], q=10, precision=0, retbins=True)\n",
    "density_bin_contingency_table = create_claimed_contingency_table('density_bin')\n",
    "claim_ratio = density_bin_contingency_table['ratio']\n",
    "claim_ratio.index = claim_ratio.index.astype(str).str.replace('\\.0', '', regex=True)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(claim_ratio)), claim_ratio)\n",
    "plt.xlabel('Age Bins')\n",
    "plt.ylabel('Claim Ratio')\n",
    "plt.title('Claim Ratio by Age')\n",
    "plt.xticks(range(len(claim_ratio)), claim_ratio.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rural(_df):\n",
    "    return _df['density'] <= 445\n",
    "\n",
    "df_train['is_rural'] = is_rural(df_train)\n",
    "rural_contingency_table = create_claimed_contingency_table('is_rural')\n",
    "rural_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_urban(_df):\n",
    "    return _df['density'] > 10521\n",
    "\n",
    "df_train['is_urban'] = is_urban(df_train)\n",
    "urban_contingency_table = create_claimed_contingency_table('is_urban')\n",
    "urban_contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim ration of nearest policies (by zipcode lat/lng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "NUM_NEIGHBORS = 40\n",
    "MAX_USER_POLICIES = df_train['user_id'].value_counts().iloc[0]\n",
    "knn_model = NearestNeighbors(n_neighbors=NUM_NEIGHBORS + MAX_USER_POLICIES, algorithm='auto')\n",
    "knn_model.fit(df_train[['lat','lng']])\n",
    "\n",
    "def predict_value(_df):\n",
    "    # Find the indices of the nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(_df[['lat','lng']])\n",
    "\n",
    "    # The policies of the same user are skipped, otherwise when the feature is built for the training data\n",
    "    # it will be polluted with the self filed_claim value (or other claims of the same user).\n",
    "    nearest_user_ids = df_train['user_id'].to_numpy()[indices]\n",
    "    policies_mask = nearest_user_ids != _df['user_id'].to_numpy()[:, np.newaxis]\n",
    "    valid_policy_indices = np.argwhere(policies_mask)\n",
    "    indices_by_row = np.split(valid_policy_indices[:, 1], np.cumsum(np.unique(valid_policy_indices[:, 0], return_counts=True)[1])[:-1])\n",
    "    selected_indices = [\n",
    "        row_indices[:NUM_NEIGHBORS]\n",
    "        for row_indices in indices_by_row\n",
    "    ]\n",
    "    \n",
    "    # Get the corresponding values\n",
    "    nearest_values = df_train['filed_claim'].to_numpy()[indices]\n",
    "    selected_values = [nearest_values[i,row_indices] for i, row_indices in enumerate(selected_indices)]\n",
    "\n",
    "    # Calculate the average value of the nearest neighbors\n",
    "    predicted_value = np.mean(selected_values, axis=1)\n",
    "    # return predicted_value\n",
    "    return np.minimum(predicted_value, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['nearest_policies_claim_ratio'] = predict_value(df_train)\n",
    "rural_contingency_table = create_claimed_contingency_table('nearest_policies_claim_ratio')\n",
    "rural_contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the baseline model based on XGBoost.  \n",
    "\n",
    "Here are some decisions I have applied to the features:\n",
    "- `state` and `postal_code` are not included, because of their high cardinality. Will explore enriching the feature set with population, density, etc. and maybe other data as enrichment in the final submission.\n",
    "- `coast`, `has_fire_alarm`, and `has_burglar_alarm` are not included, because the model performs better without them.\n",
    "- `square_ft` is not included since 97.7% of the values are missing and the claim ratio of the remaining data is not statistically significant.\n",
    "- `policy_id` and `user_id` are not included for obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'product',\n",
    "    'age_bin',\n",
    "    'card_type',\n",
    "    'fire_housing_proximity_clipped',\n",
    "    'has_fire_alarm',\n",
    "    'portable_electronics',\n",
    "    'previous_policies',\n",
    "    'previous_claims',\n",
    "    'is_rural',\n",
    "    'is_urban',\n",
    "    'nearest_policies_claim_ratio',\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'product',\n",
    "    'age_bin',\n",
    "    'card_type',\n",
    "    \"fire_housing_proximity_clipped\",\n",
    "]\n",
    "\n",
    "def transform(_df):\n",
    "    impute_missing_data(_df)\n",
    "\n",
    "    # Apply the age bins\n",
    "    _df['age_bin'] = pd.cut(_df['user_age'], bins=age_bins, include_lowest=True)\n",
    "\n",
    "    # Clip the fire-housing proximity\n",
    "    _df['fire_housing_proximity_clipped'] = _df['fire_housing_proximity'].clip(upper=4)\n",
    "\n",
    "    # Add the previous claims feature\n",
    "    prepare_previous_claims_feature(_df)\n",
    "\n",
    "    # Add the density category features\n",
    "    _df['is_rural'] = is_rural(_df)\n",
    "    _df['is_urban'] = is_urban(_df)\n",
    "\n",
    "    # Add the claim ratio of the nearest policies\n",
    "    _df['nearest_policies_claim_ratio'] = predict_value(_df)\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        _df[col] = _df[col].astype('category')\n",
    "        _df[col].cat.set_categories(df_train[col].unique())\n",
    "    return _df[features], _df['filed_claim']\n",
    "\n",
    "X, y = transform(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Evaluation\n",
    "We perform the model tuning based on the cross validation evaluation, thus we prevent overfitting against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9MsUbY5eael"
   },
   "outputs": [],
   "source": [
    "classifier = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=2,\n",
    "    # scale_pos_weight=10,\n",
    "    enable_categorical=True,\n",
    "    reg_lambda=1,\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "auc_scores = cross_val_score(classifier, X, y, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "\n",
    "print(\"Mean AUC:\", mean_auc)\n",
    "print(\"Standard Deviation of AUC:\", std_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJFatCUeae0"
   },
   "source": [
    "# Test Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJFatCUeae0"
   },
   "source": [
    "I have decided to use the AUC score (Area Under the Curve) for evaluation because the dataset is highly imbalanced.  \n",
    "The score is \"not good\" - slightly above the 0.5 random.  \n",
    "BUT... predicting claims is an extremely hard task. From an insurer's perspective, any uplift above random is very useful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_true = transform(df_test)\n",
    "predictions = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsiAbsnHeae0",
    "outputId": "a597ede8-e770-4930-c49a-17050e778064"
   },
   "outputs": [],
   "source": [
    "y_proba = predictions[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_true,  y_proba)\n",
    "auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "fig = plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=lw, label=\"AUC: %0.3f\" % auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvResVhSeae1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
