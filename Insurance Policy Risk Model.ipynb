{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDS 2024 - Instructor Guide\n",
    "\n",
    "#### The predictive task\n",
    "The task is to predict (or estimate how likely) whether a claim will be filed for a given yearly policy (binary classification).  \n",
    "The dataset contains a list of renters/home yearly policies in the US. Each yearly policy is marked whether a claim has been filed (`filed_claim` column).  \n",
    "[This](#Data-dictionary) section provides detailed descriptions for each field in the dataset.\n",
    "\n",
    "#### How the dataset challenges have been addressed\n",
    "- **Imbalanced**\n",
    "    - Stratified split - keep the positive/negative label ratio in train/test folds ([here](#Split-train/test)) and in cross-validation splits ([here](#Cross-Validation-Evaluation)).\n",
    "    - Model class weight - inflate the weight of the positive classes ([here](#Model)).\n",
    "    - Use `ROC AUC` metric because it is less sensitive to class distribution.\n",
    "- **Small**\n",
    "    - Binning of numerical features - reduces sensitivity to noise and variability.\n",
    "    - Model regularization to reduce the chance of overfitting ([here](#Model)).\n",
    "- **Missing values**\n",
    "    - All the details here: [Missing Data Analysis - Summary and Imputation](#Summary-and-Imputation).\n",
    "- **High cardinality**\n",
    "    - Enrich the policies zip code with free postal code dataset ([here](#Enrich-Postal-Codes)). Then, introduce new features: `is_rural`, `is_urban`, and `nearest_policies_claim_ratio`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMu_DfTEeaei"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "if [ ! -f data/insurance_policy_risk.csv ]; then\n",
    "    wget https://drive.usercontent.google.com/u/0/uc?id=1rBuO32aUjKyR0wPgGOqO1AjcjZBuFj9Z -O data/insurance_policy_risk.csv\n",
    "else\n",
    "    echo \"File 'data/insurance_policy_risk.csv' already exists. Skipping download.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQl1rM3reaek"
   },
   "outputs": [],
   "source": [
    "df_policies = pd.read_csv('data/insurance_policy_risk.csv', dtype={'policy_id': str, 'user_id': str})\n",
    "df_policies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policies.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `policy_id` - unique identifier of a yearly policy.\n",
    "- `user_id` - unique identifier of a user. More than one policy might be associated with the same user.\n",
    "- `state` - the insured US state. Categorical.\n",
    "- `postal_code` - the insured postal code. Categorical (some state/county information can be extracted). High Cardinality.\n",
    "- `product` - the policy type: ho4-renters / ho3-homeowners / ho6-homeowners (condo). Categorical.\n",
    "- `square_ft` - the insured property size. Numeric. 97.7% missing values.\n",
    "- `has_fire_alarm` - whether a fire alarm exists. Boolean.\n",
    "- `has_burglar_alarm` - whether a burglar alarm exists. Boolean.\n",
    "- `portable_electronics` - whether portable electronics is insured. Boolean. 3.3% missing values.\n",
    "- `coast` - Distance from the coastline. Numeric. 9.3% missing values.\n",
    "- `fire_housing_proximity` - Proximity of fire housing. Ordinal.\n",
    "- `previous_policies` - number of previous policies. Numeric.\n",
    "- `user_age` - the age of the user in years. Numeric. 0.7% missing values.\n",
    "- `card_type` - the credit card being used. Categorical. 0.4% missing values.\n",
    "- `filed_claim` - whether a user filed a claim for this policy. Boolean. Label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyCeCOuMeaek"
   },
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fire_housing_proximity` - this field was not resolved to `int` type as expected (not included in `describe` dump).  \n",
    "There are 15 instances with a character attached to a valid numeric value. It doesn't seem to correlate with anything.   \n",
    "Removing it to keep the ordinal nature of the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBbXYGGNeaek"
   },
   "outputs": [],
   "source": [
    "df_policies['fire_housing_proximity'] = df_policies['fire_housing_proximity'].str.extract('(\\d)').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich Postal Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a free postal code dataset [[1]](#References) and keep it in `data` folder.  \n",
    "Enrich the high-cardinality postal code with the following data:\n",
    "- `zip` - the 5-digit zip code assigned by the U.S. Postal Service\n",
    "- `lat` - the latitude of the zip code\n",
    "- `lng` - the longitude of the zip code\n",
    "- `city` - the official USPS city name\n",
    "- `population` - an estimate of the zip code's population\n",
    "- `density` - the estimated population per square kilometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f data/uszips.csv ]; then\n",
    "    wget https://simplemaps.com/static/data/us-zips/1.84/basic/simplemaps_uszips_basicv1.84.zip -O data/uszips.zip\n",
    "    unzip data/uszips.zip -d data\n",
    "else\n",
    "    echo \"File 'data/unzips.csv' already exists. Skipping download.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uszips = pd.read_csv('data/uszips.csv', index_col='zip', usecols=['zip','lat','lng','city','population','density'])\n",
    "\n",
    "# Impute missing zip codes with adjusted ones\n",
    "df_uszips.loc[10270] = df_uszips.loc[10128].copy()\n",
    "df_uszips.loc[60699] = df_uszips.loc[60607].copy()\n",
    "\n",
    "df = df_policies.join(df_uszips, on='postal_code', how='inner')\n",
    "\n",
    "# Verify all postal codes (of the policies) have been resolved\n",
    "assert len(df) == len(df_policies)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To prevent data leakage, all samples of the same user are within the same fold.\n",
    "1. Due to the small number of `True` labels, the label ratio is kept between the folds.\n",
    "1. The dataset is split before the EDA so any of the decisions will not rely on the data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADiAIWofeaez"
   },
   "outputs": [],
   "source": [
    "# To create a test dataset with the above directives (user in one fold, keep label ratio):\n",
    "# Constructing users list with aggregated target (True if one of the user samples is True, otherwise False).\n",
    "user_ids, labels = zip(\n",
    "    *[\n",
    "        (user_id, np.any(user_rows['filed_claim'])) for user_id, user_rows in df.groupby('user_id')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the users (70/30) while keeping the proportion of the labels (stratify).\n",
    "users_train, users_test = train_test_split(user_ids, test_size=0.3, stratify=labels, random_state=42)\n",
    "\n",
    "df_train = df[df.user_id.isin(users_train)]\n",
    "df_test = df[df.user_id.isin(users_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate train/test split.\n",
    "print(f\"Split size: {len(df_train)} + {len(df_test)} = {len(df)}\")\n",
    "print(f\"Number of users exist in both groups: {len(set(df_train['user_id']) & set(df_test['user_id']))}\")\n",
    "print(\"True target ratio in test: {:.1f}%\".format(len(df_test[df_test.filed_claim]) / 281 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`97.8%` of the policies are `ho4` (renters).  \n",
    "Claim ratio of `ho3` (homeowners) and `ho6` (condo) is a little higher, but it is not statistically significant due to the small sample size (pvalue: 0.21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_claimed_contingency_table(field):\n",
    "    contingency_table = df_train.groupby(field, observed=True).agg(total=('filed_claim', 'count'), claimed=('filed_claim', 'sum'))\n",
    "    contingency_table['unclaimed'] = contingency_table['total'] - contingency_table['claimed']\n",
    "    contingency_table['ratio'] = contingency_table['claimed'] / contingency_table['total']\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_contingency_table = create_claimed_contingency_table('product')\n",
    "product_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(product_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the policies are sold in `CA`, `TX`, and `NY`.  \n",
    "There is no statistically significant claim ratio difference between the different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_contingency_table = create_claimed_contingency_table('state')\n",
    "state_contingency_table = state_contingency_table.sort_values('total', ascending=False).head(7)\n",
    "state_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = stats.chi2_contingency(state_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He9jrgveeaet"
   },
   "source": [
    "## user_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there is a skew to the left in the distribution of user ages. This skew indicates that there are more samples with younger ages. The median age is 28.  \n",
    "Then, we use `qcut` to split the samples into 5 bins with similar sizes.  \n",
    "Examining the claim frequency for each age range reveals that users in the ages of 27 to 30 have the highest claim frequency. Claim frequency is lower the younger/older a user is (pvalue: 0.038).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['user_age'], bins=range(17, 80), edgecolor='black')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of User Age')\n",
    "plt.xticks(range(15, 80, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['user_age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['age_bin'], age_bins = pd.qcut(df_train['user_age'], q=5, precision=0, retbins=True)\n",
    "age_bin_contingency_table = create_claimed_contingency_table('age_bin')\n",
    "claim_ratio = age_bin_contingency_table['ratio']\n",
    "claim_ratio.index = claim_ratio.index.astype(str).str.replace('\\.0', '', regex=True)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(claim_ratio)), claim_ratio)\n",
    "plt.xlabel('Age Bins')\n",
    "plt.ylabel('Claim Ratio')\n",
    "plt.title('Claim Ratio by Age')\n",
    "plt.xticks(range(len(claim_ratio)), claim_ratio.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(age_bin_contingency_table[['claimed', 'unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Is card type correlative with claimed policies?* **Yes**  \n",
    "Policies with different card types have different likelihood to be claimed. Chi-squared test Null hypothesis has been rejected (pvalue: 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_type_contingency_table = create_claimed_contingency_table('card_type')\n",
    "card_type_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(card_type_contingency_table[['claimed','unclaimed']], correction=False)\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fire_housing_proximity + has_fire_alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fire-housing proximity value is clipped at `4`, because there is not enough data for values 4+.  \n",
    "No statistical difference between claimed/unclaimed ratios. Chi-squared test Null hypothesis has not been rejected (pvalue: 0.59)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fire_housing_proximity_clipped'] = df_train['fire_housing_proximity'].clip(upper=3)\n",
    "fire_housing_contingency_table = df_train.groupby(['fire_housing_proximity_clipped', 'has_fire_alarm'], observed=True).agg(\n",
    "    total=('filed_claim', 'count'), claimed=('filed_claim', 'sum')\n",
    ")\n",
    "fire_housing_contingency_table['unclaimed'] = fire_housing_contingency_table['total'] - fire_housing_contingency_table['claimed']\n",
    "fire_housing_contingency_table['ratio'] = fire_housing_contingency_table['claimed'] / fire_housing_contingency_table['total']\n",
    "fire_housing_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(fire_housing_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## portable_electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies with coverage for portable electronics are more likely to be claimed.  \n",
    "Interestingly, there is a very high correlation of missing value to `filed_claim`. The mechanism behind the missing value is not clear, but we might want to keep this indication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['portable_electronics_imputed'] = df_train['portable_electronics'].fillna('missing')\n",
    "portable_electronics_contingency_table = create_claimed_contingency_table('portable_electronics_imputed')\n",
    "portable_electronics_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.chi2_contingency(portable_electronics_contingency_table[['claimed','unclaimed']])\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing user_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_user_age_count = df_train['user_age'].isnull().sum()\n",
    "missing_portable_electronics_count = df_train['portable_electronics'].isnull().sum()\n",
    "missing_user_age_and_portable_electronics_count = (df_train['portable_electronics'].isnull() & df_train['user_age'].isnull()).sum()\n",
    "\n",
    "print(f\"\"\"\n",
    "user_age missing values are correlated with portable_electronics missingness.\n",
    "Overall user_age missing values: {missing_user_age_count} ({round(missing_user_age_count / len(df_train) * 100, 1)}% of total).\n",
    "If portable_electronics is missing: 52 ({\n",
    "    round(missing_user_age_and_portable_electronics_count / missing_portable_electronics_count * 100, 1)\n",
    "    }% of {missing_portable_electronics_count} records with missing portable_electronics values).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhaQllnqeaes"
   },
   "source": [
    "#### Missing coast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `coast` field represents the distance from the coastline: varies from 0 (San Francisco) to 30 (Sacramento).  \n",
    "Missing values are attributed to inland states.  \n",
    "Impute the missing values with 30, which reflects the same coastline risk as the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxi8sMvJeaes",
    "outputId": "8bf60514-427c-47cb-cec2-a645fb7f529a"
   },
   "outputs": [],
   "source": [
    "for coast, records in df_train[df_train.state == 'CA'].groupby('coast'):\n",
    "    first = records.iloc[0]\n",
    "    zipcode_info = df_uszips.loc[first.postal_code]\n",
    "    print(f'{int(first.coast)}: {zipcode_info.city}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"coast is null in inland states: \" + ','.join(df_train[df_train.coast.isnull()].state.unique()))\n",
    "print(\"coast is not null in states with coasts: \" + ','.join(df_train[df_train.coast.notnull()].state.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut_YLQGJeaeu"
   },
   "source": [
    "- `card_type` - missing values cannot be attributed to any cause. Assuming MCAR missing mechanism. Impute with the most frequent value (`debit`).\n",
    "- `coast` - missing values are attributed to inland states (see EDA) (MAR missing mechanism). Impute with `30`, which is the maximum value (most far from the coastline).\n",
    "- `user_age` - missing values are correlated with `portable_electronics` missingness (see above), so there is an underline cause that is unknown and likely related to the data collection (MAR missing mechanism). Impute with median age.\n",
    "- `portable_electronics` (see `user_age`) - since the missing value is more indicative of filing a claim, it will be imputed with value `1.0` which is also correlated with claim filing.\n",
    "- `square_ft` - missing value highly correlated with `ho4` (renter) product (MAR missing mechanism). Dropping this feature, since 97.7% of the values are missing and the claim ratio of the remaining data is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = df_train['user_age'].median()\n",
    "\n",
    "def impute_missing_data(_df):\n",
    "    _df.fillna(\n",
    "        {\n",
    "            'card_type': 'debit',\n",
    "            'coast': 30,\n",
    "            'user_age': median_age,\n",
    "            'portable_electronics': 1.0,\n",
    "        },\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Claim History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_previous_claims_feature(_df):\n",
    "    previous_claims = 0\n",
    "    prev_user_id = None\n",
    "    for ix, row in _df.sort_values(['user_id', 'previous_policies']).iterrows():\n",
    "        user_id = row['user_id']\n",
    "        if prev_user_id != user_id:\n",
    "            previous_claims = 0\n",
    "            \n",
    "        _df.loc[ix, 'previous_claims'] = previous_claims\n",
    "        previous_claims += row['filed_claim']\n",
    "        prev_user_id = user_id\n",
    "\n",
    "prepare_previous_claims_feature(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_claims_contingency_table = create_claimed_contingency_table('previous_claims')\n",
    "previous_claims_contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['density_bin'], density_bins = pd.qcut(df_train['density'], q=10, precision=0, retbins=True)\n",
    "density_bin_contingency_table = create_claimed_contingency_table('density_bin')\n",
    "claim_ratio = density_bin_contingency_table['ratio']\n",
    "claim_ratio.index = claim_ratio.index.astype(str).str.replace('\\.0', '', regex=True)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(range(len(claim_ratio)), claim_ratio)\n",
    "plt.xlabel('Age Bins')\n",
    "plt.ylabel('Claim Ratio')\n",
    "plt.title('Claim Ratio by Age')\n",
    "plt.xticks(range(len(claim_ratio)), claim_ratio.index, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rural(_df):\n",
    "    return _df['density'] <= 445\n",
    "\n",
    "df_train['is_rural'] = is_rural(df_train)\n",
    "rural_contingency_table = create_claimed_contingency_table('is_rural')\n",
    "rural_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_urban(_df):\n",
    "    return _df['density'] > 10521\n",
    "\n",
    "df_train['is_urban'] = is_urban(df_train)\n",
    "urban_contingency_table = create_claimed_contingency_table('is_urban')\n",
    "urban_contingency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claim ratio of nearest policies (by zipcode lat/lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is the claim ratio of the nearest 40 policies.  \n",
    "The feature is based on KNN - using the euclidean distances between policies as reflected from the lat/lng properties.  \n",
    "The complex part is to exclude the user's polcicies for each policy candidate (to prevent data pollution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "NUM_NEIGHBORS = 40\n",
    "MAX_USER_POLICIES = df_train['user_id'].value_counts().iloc[0]\n",
    "knn_model = NearestNeighbors(n_neighbors=NUM_NEIGHBORS + MAX_USER_POLICIES, algorithm='auto')\n",
    "knn_model.fit(df_train[['lat','lng']])\n",
    "\n",
    "def predict_value(_df):\n",
    "    # Find the indices of the nearest neighbors\n",
    "    _, indices = knn_model.kneighbors(_df[['lat','lng']])\n",
    "\n",
    "    # The policies of the same user are skipped, otherwise when the feature is built for the training data\n",
    "    # it will be polluted with the self filed_claim value (or other claims of the same user).\n",
    "    nearest_user_ids = df_train['user_id'].to_numpy()[indices]\n",
    "    policies_mask = nearest_user_ids != _df['user_id'].to_numpy()[:, np.newaxis]\n",
    "    valid_policy_indices = np.argwhere(policies_mask)\n",
    "    indices_by_row = np.split(valid_policy_indices[:, 1], np.cumsum(np.unique(valid_policy_indices[:, 0], return_counts=True)[1])[:-1])\n",
    "    selected_indices = [\n",
    "        row_indices[:NUM_NEIGHBORS]\n",
    "        for row_indices in indices_by_row\n",
    "    ]\n",
    "    \n",
    "    # Get the corresponding values\n",
    "    nearest_values = df_train['filed_claim'].to_numpy()[indices]\n",
    "    selected_values = [nearest_values[i,row_indices] for i, row_indices in enumerate(selected_indices)]\n",
    "\n",
    "    # Calculate the average value of the nearest neighbors\n",
    "    predicted_value = np.mean(selected_values, axis=1)\n",
    "    # return predicted_value\n",
    "    return np.minimum(predicted_value, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['nearest_policies_claim_ratio'] = predict_value(df_train)\n",
    "nearest_policies_claim_ratio = create_claimed_contingency_table('nearest_policies_claim_ratio')\n",
    "nearest_policies_claim_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is based on XGBoost [[2]](#References) with a few enhancements over the baseline model:\n",
    "- Increased number of estimators\n",
    "- Regularization [[3]](#References):\n",
    "    - Limit the XGBoost max depth to 2 (default is `6`)\n",
    "    - Limit child weight (`min_child_weight`) to 30 (default is `1`)\n",
    "- Imbalanced data adjustment: `scale_pos_weight` [[4]](#References)\n",
    "\n",
    "In addition, some fine-tuning applied to the model features:\n",
    "- `state` and `postal_code` are not included, because of their high cardinality. Instead, introduced features based on the enrichment of the zip code.\n",
    "- `coast` and `has_burglar_alarm` are not included, because the model performs better without them.\n",
    "- `square_ft` is not included since 97.7% of the values are missing and the claim ratio of the remaining data is not statistically significant.\n",
    "- `policy_id` and `user_id` are not included for obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'product',\n",
    "    'age_bin',\n",
    "    'card_type',\n",
    "    'has_burglar_alarm',\n",
    "    'fire_housing_proximity_clipped',\n",
    "    'has_fire_alarm',\n",
    "    'portable_electronics',\n",
    "    'previous_policies',\n",
    "    'is_rural',\n",
    "    'is_urban',\n",
    "    'nearest_policies_claim_ratio',\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    'product',\n",
    "    'age_bin',\n",
    "    'card_type',\n",
    "]\n",
    "\n",
    "def transform(_df):\n",
    "    impute_missing_data(_df)\n",
    "\n",
    "    # Apply the age bins\n",
    "    _df['age_bin'] = pd.cut(_df['user_age'], bins=age_bins, include_lowest=True)\n",
    "\n",
    "    # Clip the fire-housing proximity\n",
    "    _df['fire_housing_proximity_clipped'] = _df['fire_housing_proximity'].clip(upper=3)\n",
    "\n",
    "    # Add the previous claims feature\n",
    "    prepare_previous_claims_feature(_df)\n",
    "\n",
    "    # Add the density category features\n",
    "    _df['is_rural'] = is_rural(_df)\n",
    "    _df['is_urban'] = is_urban(_df)\n",
    "\n",
    "    # Add the claim ratio of the nearest policies\n",
    "    _df['nearest_policies_claim_ratio'] = predict_value(_df)\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        _df[col] = _df[col].astype('category')\n",
    "        _df[col].cat.set_categories(df_train[col].unique())\n",
    "    return _df[features], _df['filed_claim']\n",
    "\n",
    "X, y = transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_value_counts = y.value_counts().tolist()\n",
    "class_weight = label_value_counts[0] / label_value_counts[1] # the negative class is ~43 times more frequent than the positive class\n",
    "\n",
    "classifier = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=2,\n",
    "    min_child_weight=30,\n",
    "    scale_pos_weight=class_weight,\n",
    "    enable_categorical=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the model tuning based on the cross validation evaluation, thus we prevent overfitting against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9MsUbY5eael"
   },
   "outputs": [],
   "source": [
    "kfold = RepeatedStratifiedKFold(n_splits=3, n_repeats=10, random_state=42)\n",
    "auc_scores = cross_val_score(classifier, X, y, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "\n",
    "print(\"Mean AUC:\", mean_auc)\n",
    "print(\"Standard Deviation of AUC:\", std_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(classifier.feature_names_in_, classifier.feature_importances_), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJFatCUeae0"
   },
   "source": [
    "# Test Set Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJFatCUeae0"
   },
   "source": [
    "I have used the AUC score (Area Under the Curve) for evaluation because the dataset is highly imbalanced.  \n",
    "For a given policy, it is hard to predict whether a claim will be filed. Nevertheless, from an insurer's perspective, any uplift above random is very useful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = transform(df_test)\n",
    "predictions = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsiAbsnHeae0",
    "outputId": "a597ede8-e770-4930-c49a-17050e778064"
   },
   "outputs": [],
   "source": [
    "y_proba = predictions[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test,  y_proba)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "fig = plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=lw, label=\"AUC: %0.3f\" % auc)\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvResVhSeae1"
   },
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the shap analysis, the top features are age, claim-ratio of nearest policies, and the fire-housing proximity.  \n",
    "Features that are high risk indicators: `nearest_policies_claim_ratio` and `portable_electronics`.  \n",
    "Features that are low risk indicators: , `fire_housing_proximity` and `previous_policies`.  \n",
    "Some categorical features are gray-colored (since red/blue colors are associated with +/-).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(classifier)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=classifier.feature_names_in_, plot_size=(8, 4))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examined 3 positive samples with the lowest proba (most negative predictions), and also 3 negative samples with the highest proba (most positive predictions).\n",
    "\n",
    "#### Interesting insights from the false negatives analysis (diagrams 1-3):\n",
    "- Highest `fire_housing_proximity` (value: 3) significantly contributed to a predicted lower risk. This finding is somehow surprising since `fire_housing_proximity` is a high risk indicator, but with a `has_fire_alarm=True` it's actually correlated with low risk (see EDA).\n",
    "- High `nearest_policies_claim_raio` (value: 0.05) contributed to a predicted lower risk. The reason is unclear. It might suggest a model overfitting case that in some narrow cases (with other features) this value is mistakenly associated with low risk.\n",
    "- As expected, lowest and highest age buckets contributed to a predicted lower risk.\n",
    "\n",
    "#### Interesting insights from the false positives analysis (diagrams 4-6):\n",
    "- Surprisingly `has_burglar_alarm=True` contributed to a predicted high risk. The reason is unclear and should be investigated.\n",
    "- As expected, certain age buckets contributed to a predicted high risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_y = list(zip(range(len(y_test)), y_proba, y_test))\n",
    "examined_positive_samples = sorted([(proba, iloc, 1) for iloc, proba, label in zipped_y if label==1])[:3]\n",
    "examined_negative_samples = sorted([(proba, iloc, 0) for iloc, proba, label in zipped_y if label==0], reverse=True)[:3]\n",
    "examined_samples = examined_positive_samples + examined_negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer(X_test)\n",
    "num_figs = len(examined_samples)\n",
    "fig, axes = plt.subplots(num_figs, 1)\n",
    "\n",
    "for i, (proba, iloc, label) in enumerate(examined_samples):\n",
    "    plt.sca(axes[i])\n",
    "    shap.plots.waterfall(shap_values[iloc], show=False, max_display=12)\n",
    "    axes[i].set_title(f\"Label: {label} - Proba {proba:.4f}\")\n",
    "\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(10)\n",
    "fig.tight_layout(pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] [US Zip Codes Database](https://simplemaps.com/data/us-zips)\n",
    "\n",
    "\\[2\\] [XGBClassifier API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)\n",
    "\n",
    "\\[3\\] [XGBoost Control Overfitting](https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html#control-overfitting)\n",
    "\n",
    "\\[4\\] [How to Configure XGBoost for Imbalanced Classification](https://machinelearningmastery.com/xgboost-for-imbalanced-classification/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
